import json
import pandas as pd
from pycocotools.coco import COCO
from pycocoevalcap.eval import COCOEvalCap
import sys

# --- CONFIGURATION ---
DATA_DIR = "./data/coco"
# This file contains the human-written reference captions (Ground Truth)
VAL_ANNOTATIONS_PATH = f'{DATA_DIR}/annotations/annotations/captions_val2017.json'

# These are the files generated by your two pipeline scripts
ZERO_CAP_RESULTS_PATH = './results/original_zerocap_final.json'
HYBRID_VLM_RESULTS_PATH = './results/hybrid_final.json'

# --- 1. Load Ground Truth Data ---
try:
    # Initialize COCO ground truth object
    coco_gt = COCO(VAL_ANNOTATIONS_PATH)
except Exception as e:
    print(f"Error loading COCO ground truth file: {VAL_ANNOTATIONS_PATH}. Ensure the path is correct.")
    print(f"Details: {e}")
    sys.exit(1)

# --- 2. Evaluation Function ---
def evaluate_model_results(gt_coco, results_path):
    """Loads results, runs COCOEvalCap, and returns the evaluation dictionary."""
    print(f"\n--- Evaluating: {results_path} ---")
    
    # 2.1 Load generated results (candidates)
    try:
        with open(results_path, 'r') as f:
            results = json.load(f)
        
        # Format check: Ensure results are in the expected format 
        # [{ "image_id": int, "caption": "string" }, ...]
        if not all('image_id' in r and 'caption' in r for r in results):
             raise ValueError("Results file is not in the correct COCO caption format.")

        # Initialize COCO results object
        coco_res = gt_coco.loadRes(results)
    except Exception as e:
        print(f"Error loading or parsing results file {results_path}: {e}")
        return None

    # 2.2 Initialize COCO evaluator
    coco_eval = COCOEvalCap(gt_coco, coco_res)

    # Filter to only the images present in the results file (recommended practice)
    coco_eval.params['image_id'] = coco_res.getImgIds()

    # 2.3 Run evaluation
    coco_eval.evaluate()
    
    return coco_eval.eval

# --- 3. Run Evaluations and Comparison ---

# Evaluate both models
metrics_zerocap = evaluate_model_results(coco_gt, ZERO_CAP_RESULTS_PATH)
metrics_hybrid = evaluate_model_results(coco_gt, HYBRID_VLM_RESULTS_PATH)

# --- 4. Comparison Table ---
if metrics_zerocap and metrics_hybrid:
    # Create a DataFrame for easy comparison and display
    comparison_data = {
        'Metric': list(metrics_zerocap.keys()),
        'ZeroCap Baseline': [f"{v:.3f}" for v in metrics_zerocap.values()],
        'Hybrid VLM Pipeline': [f"{v:.3f}" for v in metrics_hybrid.values()]
    }
    
    df_comparison = pd.DataFrame(comparison_data)
    
    # Reorder metrics to put CIDEr (consensus metric) first
    df_comparison = df_comparison.set_index('Metric').reindex(['CIDEr', 'SPICE', 'CLIPScore', 'BLEU-4', 'METEOR', 'ROUGE_L']).reset_index()
    
    print("\n" + "="*50)
    print("üèÜ FINAL MODEL COMPARISON")
    print("="*50)
    print(df_comparison.to_markdown(index=False))
    print("\nNote: CIDEr is the primary consensus metric for COCO.")